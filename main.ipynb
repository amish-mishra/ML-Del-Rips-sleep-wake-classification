{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applicability of training ML model with TDA using Delaunay-Rips complex vs. using Rips vs. using Alpha\n",
    "Author: Amish Mishra  \n",
    "Date: November 1, 2022\n",
    "\n",
    "## Notes\n",
    "* Ensure the README.md has been followed for installation and setup before running this notebook\n",
    "* We will use DR for \"Delaunay-Rips\"\n",
    "* We will refer to the pipeline that uses DR, Rips, or Alpha for training/validating the corresponding ML model as the \"DR method\", \"Rips method\", or \"Alpha method\", respectively.\n",
    "* Rename folders with 1, 2, 3,... ahead of them to show what order they are used in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cechmate as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from ripser import ripser\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from scipy.stats import median_test\n",
    "from persistence_stats import generate_training_validation_pers_stats\n",
    "from train_ml_classifiers import train_ml_classifiers\n",
    "from validate_ml_classifiers import validate_ml_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Persistence Statistics from Persistence Diagrams using DR, Rips, and Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['Training', 'Validation']\n",
    "methods = ['rips', 'alpha', 'del_rips']\n",
    "for t in types:\n",
    "    for m in methods:\n",
    "        generate_training_validation_pers_stats(type_of_data=t, method=m, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train ML models (SVM) based on Persistence Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_arr = ['rips', 'alpha', 'del_rips']\n",
    "for func in func_arr:\n",
    "    train_ml_classifiers(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_arr = ['rips', 'alpha', 'del_rips']\n",
    "for func in func_arr:\n",
    "    validate_ml_classifiers(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the median and IQR for each method's performance metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_arr = ['rips', 'alpha', 'del_rips']\n",
    "all_perf_stats_by_func = {'rips':0, 'alpha':0, 'del_rips':0}\n",
    "\n",
    "for func in func_arr:\n",
    "    print(f'========== {func} performance ==========')\n",
    "    perf_metrics = pandas.read_pickle(\n",
    "        f'performance_metrics_tables/perf_metrics_{func}_svm_classifier.pkl')\n",
    "    summary_metrics = pandas.DataFrame({'median':[], 'iqr':[]})\n",
    "    summary_metrics['median'] = perf_metrics.median(axis=1)\n",
    "    quantile_75 = perf_metrics.quantile(0.75, axis=1)\n",
    "    quantile_25 = perf_metrics.quantile(0.25, axis=1)\n",
    "    summary_metrics['iqr'] = quantile_75 - quantile_25\n",
    "    relavant_summary_metrics = summary_metrics.iloc[4:] # The median and IQR of the confusion matrix elements are not relevant\n",
    "    all_perf_stats_by_func[func] = perf_metrics.iloc[4:]\n",
    "    print(relavant_summary_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a row-by-row median test pairwise between DR method, Rips method, and Alpha method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val_df = pandas.DataFrame({'p-value for rips vs alpha':[], 'p-value for rips vs del-rips':[],'p-value for alpha vs del-rips':[]})\n",
    "for idx, row in all_perf_stats_by_func['rips'].iterrows():\n",
    "    rips_row = all_perf_stats_by_func['rips'].loc[[idx]].values[0]\n",
    "    alpha_row = all_perf_stats_by_func['alpha'].loc[[idx]].values[0]\n",
    "    del_rips_row = all_perf_stats_by_func['del_rips'].loc[[idx]].values[0]\n",
    "    _, p_r_a, _, _ = median_test(rips_row, alpha_row)\n",
    "    _, p_r_d, _, _ = median_test(rips_row, del_rips_row)\n",
    "    _, p_a_d, _, _ = median_test(alpha_row, del_rips_row)\n",
    "    p_val_df.loc[len(p_val_df.index)] = [p_r_a, p_r_d, p_a_d]\n",
    "p_val_df.index = all_perf_stats_by_func['rips'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_val_df[['p-value for rips vs del-rips', 'p-value for alpha vs del-rips']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "A p-value smaller than 0.2 tells us that there is a significant difference between the medians for the corresponding performance metrics between the two filtration-functions-based classification models. The p-value for the aps metric for rips vs del-rips classifiers is the only one well below 0.2. This means we have sufficient evidence to suggest that the medians of the aps performance metrics for the rips-based classifier and the del-rips-based classifier are significantly different. However for the rest, notice that the p-values in table above are well above 0.2. This suggests that we cannot conclude the medians for each metric are not the same for our classification task using Delaunay-Rips or one of the other methods. **Hence, when looking at any of the performance metrics that interest us (except for aps), training a classifier using statistics generated using the Delaunay-Rips complex will perform satisfyingly as good as using either Rips or Alpha as the underlying method for persistent homology.**\n",
    "\n",
    "Based on this project, we make the following suggestion. \n",
    "\n",
    "A data analyst would benefit from making use of the Delaunay-Rips Complex in their data analysis application when\n",
    "1. Topological features of the dataset are of high interest\n",
    "2. Computation time is an essential resource \n",
    "3. Dimension of the input data is not too high/low (between 3 and 8)\n",
    "4. Average Precision Score (APS) of an ML model is not crucially important"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml_dr_sleep_wake0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "59b34ea5ac6afe5ddea2c0aee729a2ea4df3ebcb2395253ed6581abc517518d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
